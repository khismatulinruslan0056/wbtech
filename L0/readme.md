
# Демонстрационный сервис с Kafka, PostgreSQL и кэшем

Этот проект представляет собой микросервис на Go, который демонстрирует работу с очередью сообщений (Kafka), базой данных (PostgreSQL) и in-memory кэшированием. Сервис получает данные о заказах из топика Kafka, сохраняет их в базу данных и кэширует для быстрого доступа через HTTP API.

## Оглавление
- [Архитектура и структура проекта](#архитектура-и-структура-проекта)
- [Ключевые особенности](#ключевые-особенности)
- [Стек технологий](#стек-технологий)
- [Быстрый старт](#быстрый-старт)
- [Использование](#использование)
  - [Публикация данных в Kafka](#публикация-данных-в-kafka)
  - [Получение данных через API](#получение-данных-через-api)
  - [Веб-интерфейс](#веб-интерфейс)
- [Команды Makefile](#команды-makefile)
- [Тестирование](#тестирование)
- [Завершение работы](#завершение-работы)

## Архитектура и структура проекта

Проект следует принципам чистой архитектуры, разделяя логику на слои для лучшей тестируемости и поддержки.

```
.
├── cmd/                # Точки входа приложений (сервис и импортер)
│   ├── importer/         # CLI-утилита для отправки тестовых данных в Kafka
│   └── api/          # Основной сервис
├── internal/           # Внутренняя логика приложения
│   ├── app/              # Слой приложения, связывающий компоненты
│   ├── cache/            # Реализация in-memory кэша (LFU)
│   ├── config/           # Конфигурация приложения
│   ├── models/           # Модели данных (доменные структуры)
│   ├── service/          # Бизнес-логика сервиса
│   ├── storage/          # Абстракции и реализации работы с БД
│   └── transport/        # Компоненты для взаимодействия с внешним миром (HTTP, Kafka)
├── migrations/         # SQL-миграции базы данных (goose)
├── web/                # Статические файлы для веб-интерфейса (HTML, CSS, JS)
├── docker-compose.yml  # Определение сервисов для локального развертывания
├── Dockerfile          # Multi-stage Dockerfile для сборки приложений
└── MAKEFILE            # Набор команд для управления проектом
```

## Ключевые особенности

-   **Асинхронная обработка данных**: Получение заказов из Kafka в режиме реального времени.
-   **Персистентное хранение**: Сохранение валидных данных в PostgreSQL.
-   **In-Memory кэширование**: Реализован LFU-кэш для быстрого доступа к часто запрашиваемым данным.
-   **Восстановление кэша**: При старте сервиса кэш заполняется данными из БД для обеспечения отказоустойчивости.
-   **Обработка ошибок**: Невалидные сообщения из Kafka логируются и отправляются в Dead Letter Queue (DLQ) для последующего анализа.
-   **Транзакционность**: Операции с базой данных выполняются в транзакциях для обеспечения целостности данных.
-   **RESTful API**: HTTP-эндпоинт для получения данных о заказе по его ID.
-   **Простой UI**: Веб-страница для удобного запроса данных о заказе.
-   **Контейнеризация**: Весь стек (приложение, БД, брокер) запускается с помощью Docker Compose.

## Стек технологий

-   **Язык**: Go
-   **База данных**: PostgreSQL 15
-   **Брокер сообщений**: Apache Kafka
-   **Контейнеризация**: Docker, Docker Compose
-   **Миграции**: Goose
-   **HTTP-роутер**: стандартная библиотека net/http
-   **Валидация**: go-playground/validator

## Быстрый старт

Для запуска проекта вам понадобятся `Docker`, `Docker Compose` и `make`.

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <your-repo-url>
    cd <repo-name>
    ```

2.  **Создайте файл с переменными окружения:**
    Подготовьте файл .env (в корне проекта), пример:
    ```env
    # PostgreSQL (используется приложением и миграциями)
    DSN_USER=l0_user
    DSN_PASSWORD=l0_password
    DSN_NAME=l0_db
    DSN_HOST=localhost
    DSN_PORT=5432
    # Полный DSN для инструментов (psql/goose). Для контейнера хост — postgres.
    DATABASE_URL=postgres://${DSN_USER}:${DSN_PASSWORD}@postgres:5432/${DSN_NAME}?sslmode=disable

    # HTTP
    HTTP_ADDR=localhost:8081
    HTTP_TIMEOUT=4s
    HTTP_IDLE_TIMEOUT=30s
    
    # Kafka
    KAFKA_BROKERS=localhost:9092
    KAFKA_TOPIC=orders
    KAFKA_GROUP_ID=orders-group
    KAFKA_DLQ_TOPIC=orders-dlq
    KAFKA_PORT=9092
    
    # Кеш (опционально, есть значения по умолчанию)
    CACHE_TTL=60s
    CACHE_CAPACITY=10
    ```
3.  **Запустите весь стек:**
    Эта команда соберет Docker-образы, поднимет все сервисы (PostgreSQL, Kafka, Zookeeper), применит миграции базы данных и запустит приложение.
    ```bash
    make compose-up
    ```
    Процесс может занять несколько минут при первом запуске.

## Использование

После успешного запуска стек будет полностью готов к работе.

### Публикация данных в Kafka

Для эмуляции отправки сообщений в Kafka можно использовать специальный сервис-импортер. Он сгенерирует валидные и невалидные JSON-сообщения и отправит их в топик `user-events`.

```bash
make importer-run
```

Вы также можете отправлять сообщения вручную через консольный продюсер:
```bash
make kafka-produce
```

### Получение данных через API

Сервис предоставляет HTTP-эндпоинт для получения данных по `order_uid`.

-   **Успешный запрос (данные есть в кэше/БД):**
    ```bash
    # Используйте Makefile-команду
    make test-get-ok

    # Или выполните curl напрямую
    curl -i "http://localhost:8081/order/b563feb7b2b84b6test"
    ```

-   **Запрос с несуществующим ID:**
    ```bash
    make test-get-not-found
    ```

-   **Запрос с невалидным ID:**
    ```bash
    make test-get-bad-id
    ```

### Веб-интерфейс

Простой веб-интерфейс доступен в браузере по адресу:
[http://localhost:8081/](http://localhost:8081/)

Введите `order_uid` (например, `b563feb7b2b84b6test`) в поле ввода и нажмите кнопку, чтобы получить информацию о заказе.

## Команды Makefile

`Makefile` содержит удобные команды для управления жизненным циклом проекта.

-   `make help` - Показать список всех доступных команд.

**Docker Compose:**
-   `make compose-up` - Собрать образы и поднять все контейнеры в фоне.
-   `make compose-down` - Остановить и удалить контейнеры.
-   `make compose-down-v` - Остановить контейнеры и удалить все volumes (данные БД будут потеряны!).
-   `make logs-app` - Показать логи приложения.

**База данных и миграции:**
-   `make migrate-up` - Накатить все доступные миграции.
-   `make migrate-down` - Откатить последнюю миграцию.
-   `make migrate-status` - Показать статус миграций.
-   `make psql` - Подключиться к базе данных через `psql`.

**Kafka:**
-   `make kafka-topics` - Показать список топиков в Kafka.
-   `make kafka-produce` - Запустить интерактивный продюсер для отправки сообщений.
-   `make kafka-consume` - Запустить консюмер для чтения сообщений из топика.

**Тестирование API:**
-   `make test-get-ok` - Отправить успешный GET-запрос.
-   `make test-get-not-found` - Отправить запрос с несуществующим ID.

## Тестирование

В проекте реализованы как юнит-, так и интеграционные тесты для обеспечения качества и надежности кода.

### Юнит-тесты

Юнит-тесты покрывают отдельные компоненты (пакеты) в изоляции от внешних зависимостей. Для этого активно используются моки, сгенерированные с помощью `mockery`.

-   **Что тестируется:** бизнес-логика сервиса, обработчики HTTP-запросов, логика кэширования (LFU), мапперы, валидаторы.
-   **Примеры:**
    -   `internal/cache/lfu/lfu_test.go` - тестирует алгоритм LFU-кэша.
    -   `internal/service/service_test.go` - тестирует основную бизнес-логику с использованием моков для хранилища и кэша.
    -   `internal/transport/httpserver/handlers/handlers_test.go` - тестирует HTTP-хендлеры, проверяя корректность кодов ответа и тела.

### Интеграционные тесты

Интеграционные тесты проверяют взаимодействие нескольких компонентов системы, включая внешние сервисы, такие как база данных.

-   **Что тестируется:** корректность работы слоя хранения данных (`storage`) с реальной базой данных PostgreSQL.
-   **Пример:**
    -   `internal/storage/pq/pq_test.go` - эти тесты требуют запущенного экземпляра PostgreSQL. Они проверяют правильность выполнения SQL-запросов (вставка, выборка) и корректность маппинга данных.

### Как запустить тесты

Для запуска всех тестов в проекте выполните стандартную команду Go:

```bash
go test -v ./...
```
Для просмотра покрытия тестами:
```bash
go test -coverprofile=coverage.out ./... && go tool cover -html=coverage.out
```

## Завершение работы

Чтобы остановить все запущенные контейнеры и очистить ресурсы:
```bash
make compose-down
```
Если вы хотите также удалить volume'ы (например, данные PostgreSQL):
```bash
make compose-down-v
```